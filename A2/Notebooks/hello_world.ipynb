{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"novellarausell_lecture1_simple_example\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",4)\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "spark_context = spark_session.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API\n",
    "\n",
    "## Question A.1\n",
    "\n",
    "### A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in the English transcript: 1862234\n"
     ]
    }
   ],
   "source": [
    "# First we need to access the given files, which are in HDFS. The Namenode and the host name of the master containing\n",
    "# such files is 192.168.1.153\n",
    "en_lines = spark_context.textFile(\"hdfs://192.168.1.153:9000/europarl/europarl-v7.sv-en.en\")\n",
    "nr_en = en_lines.count()\n",
    "print(\"The number of lines in the English transcript: {}\".format(nr_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of lines in the Swedish transcript: 1862234\n"
     ]
    }
   ],
   "source": [
    "sv_lines = spark_context.textFile(\"hdfs://192.168.1.153:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "nr_sv = sv_lines.count()\n",
    "print(\"The number of lines in the Swedish transcript: {}\".format(nr_sv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.3 Verify that the line counts are the same for the two languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that both lengths are the same!\n",
    "assert (nr_en == nr_sv), \"Not the same length!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions in the English transcript: 2 \n",
      "\n",
      "Number of partitions in the Swedish transcript: 3 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The number of partitions is the number of blocks used by HDFS to store the file \n",
    "print(\"Number of partitions in the English transcript: {} \\n\".format(en_lines.getNumPartitions()))\n",
    "print(\"Number of partitions in the Swedish transcript: {} \\n\".format(sv_lines.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1 Pre-process the text from both RDDs by doing the following:\n",
    " - Lowercase the text\n",
    " - Tokenize the text (split on space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def cleanstring(x):\n",
    "    if isinstance(x,str):\n",
    "            return x.lower().strip().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def rddtokenizer(x):\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        return tuple((word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_tokens = sv_lines.map(lambda x: cleanstring(x)).map(lambda y: cleanstring(y))\n",
    "en_tokens = en_lines.map(lambda x: cleanstring(x)).map(lambda y: cleanstring(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English transcript inspection: \n",
      " [('resumption', 1), ('i', 1), ('although', 1), ('you', 1), ('in', 1), ('please', 1), ('the', 1), ('madam', 1), ('you', 1), ('one', 1)]\n",
      "Swedish transcript inspection: \n",
      " [('återupptagande', 1), ('jag', 1), ('som', 1), ('ni', 1), ('till', 1), ('jag', 1), ('parlamentet', 1), ('fru', 1), ('ni', 1), ('en', 1)]\n"
     ]
    }
   ],
   "source": [
    "print(\"English transcript inspection: \\n {}\".format(en_tokens.take(10)))\n",
    "print(\"Swedish transcript inspection: \\n {}\".format(sv_tokens.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that both lengths are the same!\n",
    "en_tokens.count()\n",
    "\n",
    "assert (en_tokens.count() == sv_tokens.count()), \"Not the same length!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "en_mostcommon = en_tokens.reduceByKey(add).takeOrdered(10, key = lambda x: -x[1])\n",
    "sv_mostcommon = sv_tokens.reduceByKey(add).takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 most common words on the English corpus are: \n",
      "the,i,we,it,in,this,mr,that,as,however\n",
      "The 10 most common words on the Swedish corpus are: \n",
      "jag,det,vi,i,detta,för,herr,den,de,men\n"
     ]
    }
   ],
   "source": [
    "print(\"The 10 most common words on the English corpus are: \\n\" + \",\".join([pair[0] for pair in en_mostcommon]))\n",
    "print(\"The 10 most common words on the Swedish corpus are: \\n\" + \",\".join([pair[0] for pair in sv_mostcommon]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.2 Verify that your results are reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two languages. Do this by pairing words found on short lines with the same number of words respectively. We (incorrectly) assume the words stay in the same order when translated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Key the lines by their line number (hint: ZipWithIndex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_1 = en_lines.zipWithIndex()\n",
    "sv_1 = sv_lines.zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Swap the key and value - so that the line number is the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_2 = en_1.map(lambda x: (x[1], x[0]), en_1)\n",
    "sv_2 = sv_1.map(lambda x: (x[1], x[0]), sv_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Join the two RDDs together according to the line number key, so you have pairs of matching lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_3 = en_2.join(sv_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "#### 5. Filter to leave only pairs of sentences with a small number of words per sentence, this should give a more reliable translation (you can experiment)\n",
    "#### 6. Filter to leave only pairs of sentences with the same number of words in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_456 = ensv_3.map(lambda x: (x[0],tuple(cleanstring(sentence) for sentence in x[1])))\\\n",
    ".filter(lambda x: x if x[1][0] or x[1][1] else None)\\\n",
    ".filter(lambda x: x if len(x[1][0].split(' ')) < 5 and len(x[1][1].split(' ')) < 5 else None)\\\n",
    ".filter(lambda x: x if len(x[1][0].split(' ')) == len(x[1][1].split(' ')) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. For each sentence pair, map so that you pair each (in order) word in the two sentences. We no longer need the line numbers. (hint: use python’s built in zip() function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_7 = ensv_456.flatMap(lambda x: list(zip(x[1][0].split(' '), x[1][1].split(' '))))\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Use reduce to count the number of occurrences of the word-translation-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translationtokens(x):\n",
    "    (en_word, sv_word) = x\n",
    "    return tuple((\"en: \" + en_word + \", \" + \"sv: \" + sv_word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensv_8 = ensv_7.map(translationtokens)\\\n",
    ".reduceByKey(add)\\\n",
    ".takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Print some of the most frequently occurring pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('en: applause, sv: applåder', 'frequency: 3299')\n",
      "('en: closed, sv: avslutad', 'frequency: 2854')\n",
      "('en: is, sv: är', 'frequency: 2701')\n",
      "('en: is, sv: debatten', 'frequency: 1325')\n",
      "('en: the, sv: jag', 'frequency: 1324')\n",
      "('en: debate, sv: förklarar', 'frequency: 1318')\n",
      "('en: the, sv: debatten', 'frequency: 1226')\n",
      "('en: is, sv: härmed', 'frequency: 1215')\n",
      "('en: debate, sv: är', 'frequency: 1187')\n",
      "('en: that, sv: det', 'frequency: 933')\n"
     ]
    }
   ],
   "source": [
    "print('\\n'.join([str(tuple((tpl[0], 'frequency: {}'.format(tpl[1])))) for tpl in ensv_8]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
