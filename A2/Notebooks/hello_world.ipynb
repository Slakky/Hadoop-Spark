{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"novellarausell_lecture1_simple_example\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",4)\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "spark_context = spark_session.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API\n",
    "\n",
    "## Question A.1\n",
    "\n",
    "### A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to access the given files, which are in HDFS. The Namenode and the host name of the master containing\n",
    "# such files is 192.168.1.153\n",
    "en_lines = spark_context.textFile(\"hdfs://192.168.1.153:9000/europarl/europarl-v7.sv-en.en\")\n",
    "nr_en = en_lines.count()\n",
    "print(\"The number of lines in the English transcript: {}\".format(nr_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_lines = spark_context.textFile(\"hdfs://192.168.1.153:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "nr_sv = sv_lines.count()\n",
    "print(\"The number of lines in the Swedish transcript: {}\".format(nr_sv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.3 Verify that the line counts are the same for the two languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that both lengths are the same!\n",
    "assert (nr_en == nr_sv), \"Not the same length!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of partitions is the number of blocks used by HDFS to store the file \n",
    "print(\"Number of partitions in the English transcript: {} \\n\".format(en_lines.getNumPartitions()))\n",
    "print(\"Number of partitions in the Swedish transcript: {} \\n\".format(sv_lines.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1 Pre-process the text from both RDDs by doing the following:\n",
    " - Lowercase the text\n",
    " - Tokenize the text (split on space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def cleanstring(x):\n",
    "    if isinstance(x,str):\n",
    "            return x.lower().strip().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def rddtokenizer(x):\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        return tuple((word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_tokens = sv_lines.map(lambda x: cleanstring(x)).map(lambda y: cleanstring(y))\n",
    "en_tokens = en_lines.map(lambda x: cleanstring(x)).map(lambda y: cleanstring(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English transcript inspection: \\n {}\".format(en_tokens.take(10)))\n",
    "print(\"Swedish transcript inspection: \\n {}\".format(sv_tokens.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that both lengths are the same!\n",
    "en_tokens.count()\n",
    "\n",
    "assert (en_tokens.count() == sv_tokens.count()), \"Not the same length!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "en_mostcommon = en_tokens.reduceByKey(add).takeOrdered(10, key = lambda x: -x[1])\n",
    "sv_mostcommon = sv_tokens.reduceByKey(add).takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 10 most common words on the English corpus are: \\n\" + \",\".join([pair[0] for pair in en_mostcommon]))\n",
    "print(\"The 10 most common words on the Swedish corpus are: \\n\" + \",\".join([pair[0] for pair in sv_mostcommon]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.2 Verify that your results are reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two languages. Do this by pairing words found on short lines with the same number of words respectively. We (incorrectly) assume the words stay in the same order when translated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Key the lines by their line number (hint: ZipWithIndex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_1 = en_lines.zipWithIndex()\n",
    "sv_1 = sv_lines.zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Swap the key and value - so that the line number is the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_2 = en_1.map(lambda x: (x[1], x[0]), en_1)\n",
    "sv_2 = sv_1.map(lambda x: (x[1], x[0]), sv_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Join the two RDDs together according to the line number key, so you have pairs of matching lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_3 = en_2.join(sv_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "#### 5. Filter to leave only pairs of sentences with a small number of words per sentence, this should give a more reliable translation (you can experiment)\n",
    "#### 6. Filter to leave only pairs of sentences with the same number of words in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_456 = ensv_3.map(lambda x: (x[0],tuple(cleanstring(sentence) for sentence in x[1])))\\\n",
    ".filter(lambda x: x if x[1][0] or x[1][1] else None)\\\n",
    ".filter(lambda x: x if len(x[1][0].split(' ')) < 5 and len(x[1][1].split(' ')) < 5 else None)\\\n",
    ".filter(lambda x: x if len(x[1][0].split(' ')) == len(x[1][1].split(' ')) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. For each sentence pair, map so that you pair each (in order) word in the two sentences. We no longer need the line numbers. (hint: use python’s built in zip() function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_7 = ensv_456.flatMap(lambda x: list(zip(x[1][0].split(' '), x[1][1].split(' '))))\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Use reduce to count the number of occurrences of the word-translation-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translationtokens(x):\n",
    "    (en_word, sv_word) = x\n",
    "    return tuple((\"en: \" + en_word + \", \" + \"sv: \" + sv_word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensv_8 = ensv_7.map(translationtokens)\\\n",
    ".reduceByKey(add)\\\n",
    ".takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Print some of the most frequently occurring pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([str(tuple((tpl[0], 'frequency: {}'.format(tpl[1])))) for tpl in ensv_8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B - Working with DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 Load the CSV file from HDFS, and call show() to verify the data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = spark_session.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv('hdfs://192.168.1.153:9000/parking-citations.csv')\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Print the schema for the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticket number: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Issue time: string (nullable = true)\n",
      " |-- Meter Id: string (nullable = true)\n",
      " |-- Marked Time: string (nullable = true)\n",
      " |-- RP State Plate: string (nullable = true)\n",
      " |-- Plate Expiry Date: string (nullable = true)\n",
      " |-- VIN: string (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Body Style: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Route: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Violation code: string (nullable = true)\n",
      " |-- Violation Description: string (nullable = true)\n",
      " |-- Fine amount: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Count the number of rows in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4 Count the number of partitions in the underlying RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5 Drop the columns VIN, Latitude and Longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_5 = data_frame.drop('VIN', 'Latitude', 'Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.6 Find the maximum fine amount. How many fines have this amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "udf_tofloat = func.udf(lambda x: float(x) if x else None, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_6 = data_frame_5.withColumn(\"Fine amount\", udf_tofloat(\"Fine amount\"))\n",
    "maxFine = data_frame_6.agg(func.max('Fine amount')).first()[0]\n",
    "maxFineCount = data_frame_6.filter(data_frame_6['Fine amount'] == maxFine).count()\n",
    "print(\"The maximum fine amount is: {}\\n\".format(maxFine))\n",
    "print(\"There are {} fines that have this amount\".format(maxFineCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.7 Show the top 20 most frequent vehicle makes, and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_makes = data_frame_5.groupby(\"Make\")\\\n",
    ".count()\\\n",
    ".orderBy('count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|Make|  count|\n",
      "+----+-------+\n",
      "|TOYT|1531949|\n",
      "|HOND|1043276|\n",
      "|FORD| 807498|\n",
      "|NISS| 662097|\n",
      "|CHEV| 631413|\n",
      "| BMW| 422916|\n",
      "|MERZ| 376830|\n",
      "|VOLK| 316002|\n",
      "|HYUN| 285286|\n",
      "|DODG| 271590|\n",
      "|LEXS| 263269|\n",
      "| KIA| 217795|\n",
      "|JEEP| 214965|\n",
      "|AUDI| 179718|\n",
      "|MAZD| 169811|\n",
      "|OTHR| 154376|\n",
      "| GMC| 132788|\n",
      "|INFI| 120340|\n",
      "|CHRY| 120317|\n",
      "|ACUR| 111265|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vehicle_makes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  B.8 Create a User Defined Function to create a new column, ‘color long’, mapping the original colors to their corresponding values in the dictionary below. If there is no key matching the original color, use the original color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = {\n",
    "'AL':'Aluminum', 'AM':'Amber', 'BG':'Beige', 'BK':'Black',\n",
    "'BL':'Blue', 'BN':'Brown', 'BR':'Brown', 'BZ':'Bronze',\n",
    "'CH':'Charcoal', 'DK':'Dark', 'GD':'Gold', 'GO':'Gold',\n",
    "'GN':'Green', 'GY':'Gray', 'GT':'Granite', 'IV':'Ivory',\n",
    "'LT':'Light', 'OL':'Olive', 'OR':'Orange', 'MR':'Maroon',\n",
    "'PK':'Pink', 'RD':'Red', 'RE':'Red', 'SI':'Silver', 'SL':'Silver',\n",
    "'SM':'Smoke', 'TN':'Tan', 'VT':'Violet', 'WT':'White',\n",
    "'WH':'White', 'YL':'Yellow', 'YE':'Yellow', 'UN':'Unknown'\n",
    "}\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "\n",
    "udf_expandcolor = func.udf(lambda x: COLORS.get(x, x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_8 = data_frame_5.withColumn('Color long', udf_expandcolor(data_frame_5.Color))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.9 Using this new column, what’s the most frequent colour value for Toyotas (TOYT)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent color value for Toyotas is: Gray\n"
     ]
    }
   ],
   "source": [
    "toyotas_colors_df = data_frame_8.filter(data_frame_8[\"Make\"] == 'TOYT')\\\n",
    ".groupby('Color long')\\\n",
    ".count()\\\n",
    ".orderBy(func.desc('count'))\\\n",
    ".take(1)\n",
    "print(\"The most frequent color value for Toyotas is: {}\".format(toyotas_colors_df[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
