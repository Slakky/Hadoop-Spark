{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "spark_session = SparkSession\\\n",
    "        .builder\\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"novellarausell_lecture1_simple_example\")\\\n",
    "        .config(\"spark.dynamicAllocation.enabled\", True)\\\n",
    "        .config(\"spark.shuffle.service.enabled\", True)\\\n",
    "        .config(\"spark.dynamicAllocation.executorIdleTimeout\",\"30s\")\\\n",
    "        .config(\"spark.executor.cores\",4)\\\n",
    "        .getOrCreate()\n",
    "        \n",
    "spark_context = spark_session.sparkContext\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part A - Working with the RDD API\n",
    "\n",
    "## Question A.1\n",
    "\n",
    "### A.1.1 Read the English transcripts with Spark, and count the number of lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we need to access the given files, which are in HDFS. The Namenode and the host name of the master containing\n",
    "# such files is 192.168.1.153\n",
    "en_lines = spark_context.textFile(\"hdfs://192.168.1.153:9000/europarl/europarl-v7.sv-en.en\")\n",
    "nr_en = en_lines.count()\n",
    "print(\"The number of lines in the English transcript: {}\".format(nr_en))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.2 Do the same with the other language (so that you have a separate lineage of RDDs for each)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_lines = spark_context.textFile(\"hdfs://192.168.1.153:9000/europarl/europarl-v7.sv-en.sv\")\n",
    "nr_sv = sv_lines.count()\n",
    "print(\"The number of lines in the Swedish transcript: {}\".format(nr_sv))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.3 Verify that the line counts are the same for the two languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that both lengths are the same!\n",
    "assert (nr_en == nr_sv), \"Not the same length!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.1.4 Count the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The number of partitions is the number of blocks used by HDFS to store the file \n",
    "print(\"Number of partitions in the English transcript: {} \\n\".format(en_lines.getNumPartitions()))\n",
    "print(\"Number of partitions in the Swedish transcript: {} \\n\".format(sv_lines.getNumPartitions()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.1 Pre-process the text from both RDDs by doing the following:\n",
    " - Lowercase the text\n",
    " - Tokenize the text (split on space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def cleanstring(x):\n",
    "    if isinstance(x,str):\n",
    "            return x.lower().strip().translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "def rddtokenizer(x):\n",
    "    words = x.split(' ')\n",
    "    for word in words:\n",
    "        return tuple((word,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sv_tokens = sv_lines.map(lambda x: cleanstring(x)).map(lambda y: cleanstring(y))\n",
    "en_tokens = en_lines.map(lambda x: cleanstring(x)).map(lambda y: cleanstring(y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.2 Inspect 10 entries from each of your RDDs to verify your pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"English transcript inspection: \\n {}\".format(en_tokens.take(10)))\n",
    "print(\"Swedish transcript inspection: \\n {}\".format(sv_tokens.take(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.2.3 Verify that the line counts still match after the pre-processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Asserting that both lengths are the same!\n",
    "en_tokens.count()\n",
    "\n",
    "assert (en_tokens.count() == sv_tokens.count()), \"Not the same length!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.1 Use Spark to compute the 10 most frequently according words in the English language corpus. Repeat for the other language."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "en_mostcommon = en_tokens.reduceByKey(add).takeOrdered(10, key = lambda x: -x[1])\n",
    "sv_mostcommon = sv_tokens.reduceByKey(add).takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"The 10 most common words on the English corpus are: \\n\" + \",\".join([pair[0] for pair in en_mostcommon]))\n",
    "print(\"The 10 most common words on the Swedish corpus are: \\n\" + \",\".join([pair[0] for pair in sv_mostcommon]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.3.2 Verify that your results are reasonable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question A.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A.4.1 Use this parallel corpus to mine some translations in the form of word pairs, for the two languages. Do this by pairing words found on short lines with the same number of words respectively. We (incorrectly) assume the words stay in the same order when translated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Key the lines by their line number (hint: ZipWithIndex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_1 = en_lines.zipWithIndex()\n",
    "sv_1 = sv_lines.zipWithIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Swap the key and value - so that the line number is the key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_2 = en_1.map(lambda x: (x[1], x[0]), en_1)\n",
    "sv_2 = sv_1.map(lambda x: (x[1], x[0]), sv_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Join the two RDDs together according to the line number key, so you have pairs of matching lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_3 = en_2.join(sv_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Filter to exclude line pairs that have an empty/missing “corresponding” sentence.\n",
    "#### 5. Filter to leave only pairs of sentences with a small number of words per sentence, this should give a more reliable translation (you can experiment)\n",
    "#### 6. Filter to leave only pairs of sentences with the same number of words in each sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_456 = ensv_3.map(lambda x: (x[0],tuple(cleanstring(sentence) for sentence in x[1])))\\\n",
    ".filter(lambda x: x if x[1][0] or x[1][1] else None)\\\n",
    ".filter(lambda x: x if len(x[1][0].split(' ')) < 5 and len(x[1][1].split(' ')) < 5 else None)\\\n",
    ".filter(lambda x: x if len(x[1][0].split(' ')) == len(x[1][1].split(' ')) else None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. For each sentence pair, map so that you pair each (in order) word in the two sentences. We no longer need the line numbers. (hint: use python’s built in zip() function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensv_7 = ensv_456.flatMap(lambda x: list(zip(x[1][0].split(' '), x[1][1].split(' '))))\\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. Use reduce to count the number of occurrences of the word-translation-pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translationtokens(x):\n",
    "    (en_word, sv_word) = x\n",
    "    return tuple((\"en: \" + en_word + \", \" + \"sv: \" + sv_word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "ensv_8 = ensv_7.map(translationtokens)\\\n",
    ".reduceByKey(add)\\\n",
    ".takeOrdered(10, key = lambda x: -x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. Print some of the most frequently occurring pairs of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\n'.join([str(tuple((tpl[0], 'frequency: {}'.format(tpl[1])))) for tpl in ensv_8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section B - Working with DataFrames and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.1 Load the CSV file from HDFS, and call show() to verify the data is loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame = spark_session.read\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .csv('hdfs://192.168.1.153:9000/parking-citations.csv')\\\n",
    "    .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.2 Print the schema for the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Ticket number: string (nullable = true)\n",
      " |-- Issue Date: string (nullable = true)\n",
      " |-- Issue time: string (nullable = true)\n",
      " |-- Meter Id: string (nullable = true)\n",
      " |-- Marked Time: string (nullable = true)\n",
      " |-- RP State Plate: string (nullable = true)\n",
      " |-- Plate Expiry Date: string (nullable = true)\n",
      " |-- VIN: string (nullable = true)\n",
      " |-- Make: string (nullable = true)\n",
      " |-- Body Style: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Location: string (nullable = true)\n",
      " |-- Route: string (nullable = true)\n",
      " |-- Agency: string (nullable = true)\n",
      " |-- Violation code: string (nullable = true)\n",
      " |-- Violation Description: string (nullable = true)\n",
      " |-- Fine amount: string (nullable = true)\n",
      " |-- Latitude: string (nullable = true)\n",
      " |-- Longitude: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data_frame.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.3 Count the number of rows in the CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.4 Count the number of partitions in the underlying RDD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.5 Drop the columns VIN, Latitude and Longitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_5 = data_frame.drop('VIN', 'Latitude', 'Longitude')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.6 Find the maximum fine amount. How many fines have this amount?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import FloatType\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "udf_tofloat = func.udf(lambda x: float(x) if x else None, FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_6 = data_frame_5.withColumn(\"Fine amount\", udf_tofloat(\"Fine amount\"))\n",
    "maxFine = data_frame_6.agg(func.max('Fine amount')).first()[0]\n",
    "maxFineCount = data_frame_6.filter(data_frame_6['Fine amount'] == maxFine).count()\n",
    "print(\"The maximum fine amount is: {}\\n\".format(maxFine))\n",
    "print(\"There are {} fines that have this amount\".format(maxFineCount))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B.7 Show the top 20 most frequent vehicle makes, and their frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "vehicle_makes = data_frame_5.groupby(\"Make\")\\\n",
    ".count()\\\n",
    ".orderBy('count', ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-------+\n",
      "|Make|  count|\n",
      "+----+-------+\n",
      "|TOYT|1531949|\n",
      "|HOND|1043276|\n",
      "|FORD| 807498|\n",
      "|NISS| 662097|\n",
      "|CHEV| 631413|\n",
      "| BMW| 422916|\n",
      "|MERZ| 376830|\n",
      "|VOLK| 316002|\n",
      "|HYUN| 285286|\n",
      "|DODG| 271590|\n",
      "|LEXS| 263269|\n",
      "| KIA| 217795|\n",
      "|JEEP| 214965|\n",
      "|AUDI| 179718|\n",
      "|MAZD| 169811|\n",
      "|OTHR| 154376|\n",
      "| GMC| 132788|\n",
      "|INFI| 120340|\n",
      "|CHRY| 120317|\n",
      "|ACUR| 111265|\n",
      "+----+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vehicle_makes.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  B.8 Create a User Defined Function to create a new column, ‘color long’, mapping the original colors to their corresponding values in the dictionary below. If there is no key matching the original color, use the original color"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLORS = {\n",
    "'AL':'Aluminum', 'AM':'Amber', 'BG':'Beige', 'BK':'Black',\n",
    "'BL':'Blue', 'BN':'Brown', 'BR':'Brown', 'BZ':'Bronze',\n",
    "'CH':'Charcoal', 'DK':'Dark', 'GD':'Gold', 'GO':'Gold',\n",
    "'GN':'Green', 'GY':'Gray', 'GT':'Granite', 'IV':'Ivory',\n",
    "'LT':'Light', 'OL':'Olive', 'OR':'Orange', 'MR':'Maroon',\n",
    "'PK':'Pink', 'RD':'Red', 'RE':'Red', 'SI':'Silver', 'SL':'Silver',\n",
    "'SM':'Smoke', 'TN':'Tan', 'VT':'Violet', 'WT':'White',\n",
    "'WH':'White', 'YL':'Yellow', 'YE':'Yellow', 'UN':'Unknown'\n",
    "}\n",
    "\n",
    "from pyspark.sql.types import StringType\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "\n",
    "udf_expandcolor = func.udf(lambda x: COLORS.get(x, x), StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_frame_8 = data_frame_5.withColumn('Color long', udf_expandcolor(data_frame_5.Color))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.9 Using this new column, what’s the most frequent colour value for Toyotas (TOYT)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most frequent color value for Toyotas is: Gray\n"
     ]
    }
   ],
   "source": [
    "toyotas_colors_df = data_frame_8.filter(data_frame_8[\"Make\"] == 'TOYT')\\\n",
    ".groupby('Color long')\\\n",
    ".count()\\\n",
    ".orderBy(func.desc('count'))\\\n",
    ".take(1)\n",
    "print(\"The most frequent color value for Toyotas is: {}\".format(toyotas_colors_df[0][0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section C - Concepts in Apache Spark and Distributed Computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.1 Why do we use the Map-Reduce paradigm?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Map-Reduce paradigm allow us to program in a simple and powerful framework that can handle parallelized and distributed computations. The framework is designed around two functions: map and reduce. The map function can be run in parallel on each document  and takes an input pair in order to produce an intermediate key/value pairs. The reduce function aggregates the pairs in a pairwise fashion. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.2 Anna Exampleson is trying to understand her Spark code by adding a print statement inside her split_line(..) function, as shown in this code snippet:\n",
    "\n",
    "```python\n",
    "def split_line(line):\n",
    " print('splitting line...')\n",
    " return line.split(' ')\n",
    "lines = spark_context.textFile(\"hdfs://host:9000/king-dream.txt\")\n",
    "print(lines.flatMap(split_line).take(10))\n",
    "```\n",
    "\n",
    "### When she runs this code in her notebook, she sees the following output:\n",
    "\n",
    "```bash\n",
    "['I', 'am', 'happy', 'to', 'join', 'with', 'you', 'today', 'in',\n",
    "'what']\n",
    "```\n",
    "\n",
    "\n",
    "### But, she doesn’t see the “splitting line…” output in her notebook. Why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Within the Spark terminology, Anna is executing a transformation on an rdd  (`lines` in this example) by calling the `split_line` function inside the `flatMap()` method. Logging inside of a transform operation will end up in Spark's executor logs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.3 “Calling .collect() on a large dataset can cause my driver application to run out of memory” Explain why"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.collect()` method returns all the elements in the RDD at the driver program. This driver program is the node hosting the user's Spark application. This will typically be a modest machine (top 16 gb of RAM) so if the RDD is big enough, it can cause the program to crash due to a lack of memory in this machine.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.4 Are partitions mutable? Why is this advantageous?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When an RDD is created, Spark automatically splits the data into different nodes. This process is called partitioning and each chunk of data is called a partition. \n",
    "\n",
    "While RDDs are statically typed and immutable, partitions are mutable. One can create a partition through some transformations on existing partitions. This is advantageous because it allow us to perform parallelized operations on the Dataset across the cluster while keeping the RDD untouched, safe and ready to be recovered at any moment. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C.5 In what sense are RDDs ‘resilient’? How is this achieved?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark's RDDs are resilient because they are fault-tolerant and allow the framework to restore missing or damaged partitions due to machine failures. \n",
    "\n",
    "If there's a failure on the cluster, it is pointed out by the cluster manager. Afterwards it tries to assign another worker to continue the job on the same RDD. Due to the new worker, there is no data loss. To accomplish fault tolerance for all the RDDs, the data is being replicated among many nodes in the cluster.\n",
    "\n",
    "In other words, RDDs are functions of their own input, meaning that RDDs are not really a collection of data but a way of building new data from previous data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section D - Essay Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### “A colleague has mentioned her Spark application has poor performance, what is your advice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Tuning batch size: in order to get higher performances on Spark, due to its streaming nature, we need to reduce the network cost to connect between the application and the input source as well as the output source. Another factor to consider is the data processing time, which will depend on the transformations made on the data; this can be improved with some trial and error on the actions performed. \n",
    "\n",
    "2. Tuning workers/executors: if we want to increase the performance power one of the things we can do is increase the number of workers or the number of executors per worker (this will, in turn, allocate less data partitions on each worker). We can tweak also the number of cores per executor with `spark.executors.core`, this will allow us to find a sweet spot between parallelization / processing power (adding more workers for parallelization or increasing the cores per executor for processing power).\n",
    "\n",
    "3. Tuning parallelism: this will allow us to tune the number of tasks per block. To increase the number of tasks for a given batch interval (increasing parallelism and performance), we can reduce the block interval. We can also tweak the parallelism during the data processing by configuting `spark.default.parallelism`.\n",
    "\n",
    "4. Memory tuning: when Spark receives the data if it doesn't fit the memory it is thrown to disk, decreasing performance. Enabling Kryo serialization and using `spark.rdd.compress` can further decrease memory usage. Another option is to directly increase the memory resources of the cluster. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
